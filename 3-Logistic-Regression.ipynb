{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87b2e779054ff68",
   "metadata": {},
   "source": [
    "### 2.1 问题背景\n",
    "\n",
    "##### 应用案例背景\n",
    "\n",
    "假设我们有一个数据集，包含患者的多种生理指标（如年龄、性别、体重指数（BMI）、血糖水平等）以及他们是否被诊断为糖尿病（是或否）。我们的目标是建立一个逻辑回归模型，**根据这些生理指标预测一个患者是否有糖尿病的概率。**\n",
    "\n",
    "##### 数据集表示\n",
    "\n",
    "假设我们的数据集包含以下特征：\n",
    "\n",
    "- $X_1$: 年龄\n",
    "- $X_2$: 性别（0表示女性，1表示男性）\n",
    "- $X_3$: 体重指数（BMI）\n",
    "- $X_4$: 血糖水平\n",
    "- $Y$: 是否有糖尿病（0表示没有，1表示有）\n",
    "\n",
    "##### 逻辑回归模型\n",
    "\n",
    "我们将使用逻辑回归模型来预测$P(Y=1|X)$，即给定生理指标X时，患者有糖尿病的概率。模型的形式为：\n",
    "\n",
    "$P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962746f7a44f8eb4",
   "metadata": {},
   "source": [
    "### 2.2 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b79b428f4ec59a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.685429900Z",
     "start_time": "2024-04-24T02:48:04.597574Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模拟创建数据\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 设置随机种子以保证结果可复现\n",
    "np.random.seed(0)\n",
    "# 创建数据\n",
    "data_size = 1000  # 数据集大小\n",
    "# 年龄范围从20到80\n",
    "ages = np.random.randint(20, 80, size=data_size)\n",
    "# 性别为0或1\n",
    "genders = np.random.randint(0, 2, size=data_size)\n",
    "# BMI范围从18到35\n",
    "bmis = np.random.uniform(18, 35, size=data_size)\n",
    "# 血糖水平范围从70到140\n",
    "glucose_levels = np.random.uniform(70, 140, size=data_size)\n",
    "# 指定结果：是否有糖尿病\n",
    "labels = np.random.randint(0, 2, size=data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd12ab4e7e702e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.717343Z",
     "start_time": "2024-04-24T02:48:04.690416400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Gender        BMI  Glucose_Level  Diabetes\n",
      "0   64       1  21.150926      74.332242         1\n",
      "1   67       0  34.928860     127.491243         1\n",
      "2   73       0  20.199048      96.576651         1\n",
      "3   20       1  26.014774     110.008514         1\n",
      "4   23       1  19.157583     138.848879         1\n"
     ]
    }
   ],
   "source": [
    "# 定义结果变量——先假定一个模型来生成数据的结果\n",
    "# 使用逻辑函数生成糖尿病的概率，并依此生成0或1的标签\n",
    "beta = np.array([-8, 0.05, 0.25, 0.1, 0.05])  # 假定的模型参数\n",
    "X = np.column_stack((np.ones(data_size), ages, genders, bmis, glucose_levels))\n",
    "linear_combination = X.dot(beta)\n",
    "probabilities = 1 / (1 + np.exp(-linear_combination))\n",
    "labels = np.random.binomial(1, probabilities)\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Age': ages,\n",
    "    'Gender': genders,\n",
    "    'BMI': bmis,\n",
    "    'Glucose_Level': glucose_levels,\n",
    "    'Diabetes': labels\n",
    "})\n",
    "\n",
    "# 显示前几行数据\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd2d726d2373be",
   "metadata": {},
   "source": [
    "### 2.3 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a16494fbdd6c177",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.759777Z",
     "start_time": "2024-04-24T02:48:04.711361400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    880\n",
      "0    120\n",
      "Name: Diabetes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 数据集中有多少患者被诊断为糖尿病\n",
    "print(df['Diabetes'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fe67626befdf7",
   "metadata": {},
   "source": [
    "### 2.4 手动实现逻辑回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d4d78146426e1",
   "metadata": {},
   "source": [
    "逻辑回归模型的训练过程就是最大化似然函数的过程。我们可以使用梯度下降法来最大化似然函数。下面我们将手动实现逻辑回归模型的训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ea5f56e1e126f",
   "metadata": {},
   "source": [
    "使用梯度下降来最大化逻辑回归的似然函数，实际上我们通常是在最小化对数似然函数的负值，这被称为对数损失或交叉熵损失。这种方法称为梯度下降，其目标是找到一组参数（权重和偏置），使得损失函数最小化。\n",
    "公式为：\n",
    "$$ J(\\mathbf{w}, b) = -\\sum_{i=1}^n \\left[y^{(i)} \\log(\\sigma(z^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(z^{(i)}))\\right] $$\n",
    "\n",
    "其中$\\beta_0$就是b，w就是$\\beta_1 到 \\beta_n$，z就是线性组合$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4$，$\\sigma$就是sigmoid函数。\n",
    "现在我们的目标就是最小化上面的公式，要求它的最小值对应的$\\beta$，我们就又需要用到梯度下降法了。\n",
    "在梯度下降法中，我们更新权重 $\\mathbf{w}$ 和偏置 $b$ 以最小化损失 $J$，更新规则为：\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} J$ \n",
    "\n",
    "$b \\leftarrow b - \\alpha \\frac{\\partial J}{\\partial b}$\n",
    "\n",
    "所以现在我们就先要对上面的公式求导，求出梯度，然后再用梯度下降法来更新$\\beta$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800428f63d54f90",
   "metadata": {},
   "source": [
    "### 对 $\\beta_0$ 的偏导数（对于偏置项）\n",
    "\n",
    "1. **写出损失函数 $J$：**\n",
    "   $J(w, b) = -\\sum_{i=1}^n \\left[y^{(i)} \\log(\\sigma(z^{(i)})) + (1 - y^{(i)}) \\log(1 - \\sigma(z^{(i)}))\\right]$\n",
    "2. **写出 $z^{(i)}$：**\n",
    "   $z^{(i)} = \\beta_0 + \\beta_1 x_1^{(i)} + \\ldots + \\beta_n x_n^{(i)}$\n",
    "3. **对 $z^{(i)}$ 求 $\\beta_0$ 的偏导数：**\n",
    "   $\\frac{\\partial z^{(i)}}{\\partial \\beta_0} = 1$\n",
    "4. **对 Sigmoid 函数求导：**\n",
    "   $\\frac{d\\sigma}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "   其中，$\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n",
    "5. **对损失函数 $J$ 关于 $\\sigma(z^{(i)})$ 求偏导数：**\n",
    "   $\\frac{\\partial J}{\\partial \\sigma(z^{(i)})} = -\\left(\\frac{y^{(i)}}{\\sigma(z^{(i)})} - \\frac{1 - y^{(i)}}{1 - \\sigma(z^{(i)})}\\right)$\n",
    "6. **应用链式法则得到 $J$ 关于 $\\beta_0$ 的偏导数：**\n",
    "   $\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^n \\frac{\\partial J}{\\partial \\sigma(z^{(i)})} \\cdot \\frac{d\\sigma}{dz^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial \\beta_0}$\n",
    "7. **计算最终的偏导数表达式：**\n",
    "   $\\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^n \\left(\\sigma(z^{(i)}) - y^{(i)}\\right)$\n",
    "\n",
    "### 对 $\\beta_j$ 的偏导数（对于权重项）\n",
    "\n",
    "1. **对 $z^{(i)}$ 求 $\\beta_j$ 的偏导数：**\n",
    "   $\\frac{\\partial z^{(i)}}{\\partial \\beta_j} = x_j^{(i)}$\n",
    "2. **使用链式法则得到 $J$ 关于 $\\beta_j$ 的偏导数：**\n",
    "   $\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^n \\frac{\\partial J}{\\partial \\sigma(z^{(i)})} \\cdot \\frac{d\\sigma}{dz^{(i)}} \\cdot \\frac{\\partial z^{(i)}}{\\partial \\beta_j}$\n",
    "3. **计算最终的偏导数表达式：**\n",
    "   $\\frac{\\partial J}{\\partial \\beta_j} = \\sum_{i=1}^n \\left(\\sigma(z^{(i)}) - y^{(i)}\\right) x_j^{(i)}$\n",
    "\n",
    "   在实际的算法实现中，这些导数会用来更新 $\\beta_0$ 和 $\\beta_j$：\n",
    "\n",
    "因为$\\sigma(z^{(i)})$其实就是我们的预测值，所以这个公式的意思就是我们的预测值减去实际值，然后再求和，就是我们的偏导数。\n",
    "\n",
    "$\\beta_0 := \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0}$\n",
    "$\\beta_j := \\beta_j - \\alpha \\frac{\\partial J}{\\partial \\beta_j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bda6bfc8598173",
   "metadata": {},
   "source": [
    "1. **定义 Sigmoid 函数：** 这是逻辑回归中的核心函数，将线性组合映射到 (0, 1) 区间。\n",
    "2. **定义损失函数：** 这个函数计算当前权重和偏置下的对数损失。\n",
    "3. **定义梯度计算函数：** 计算损失函数关于每个参数的梯度。\n",
    "4. **定义梯度下降更新规则：** 使用计算得到的梯度更新权重和偏置。\n",
    "5. **执行训练过程：** 迭代执行梯度下降步骤，直至满足收敛条件或达到指定的迭代次数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "584924fa9b301f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.769749Z",
     "start_time": "2024-04-24T02:48:04.724325300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义 Sigmoid 函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e611af7e0647e9f",
   "metadata": {},
   "source": [
    " 定义损失函数\n",
    " 公式为：J(w, b) = $-\\sum_{i=1}^n [y^{(i)} \\log(\\sigma(z^{(i)}) + (1 - y^{(i)}) \\log(1 - \\sigma(z^{(i))})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b2dc232de218aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.808644900Z",
     "start_time": "2024-04-24T02:48:04.765760700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义对数损失函数——我们的目的就是让损失最小，就是让似然函数最大，当求的这些beta能够使得损失最小的时候，就是我们的最优解，beta就是我们的最优参数\n",
    "def compute_loss(X, y, w, b):\n",
    "    # 对应上面的z=(β0+β1X1+β2X2+β3X3+β4X4)\n",
    "    z = np.dot(X, w) + b \n",
    "    probs = sigmoid(z)\n",
    "    # np.mean()求损失的均值\n",
    "    loss = -np.mean(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92ebc2139f7ae86",
   "metadata": {},
   "source": [
    "在 compute_gradients 函数中，参数的含义如下：\n",
    "\n",
    "X: 特征数据矩阵。它是一个二维数组，其中每一行代表一个训练样本，每一列代表一个特征。\n",
    "y: 标签向量。它是一个一维数组，包含与特征矩阵 X 中的每一行相对应的标签。在二分类逻辑回归中，这些标签通常是 0 或 1。\n",
    "w: 权重向量。这是一个一维数组，其中每个元素对应于 X 矩阵中某个特征的权重。\n",
    "b: 偏置项。这是一个标量，它与权重向量 w 一起决定了模型的预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf25ac86770bd3ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.862500800Z",
     "start_time": "2024-04-24T02:48:04.812635200Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义梯度计算函数——这个函数就是对上面的损失函数求导，求出梯度\n",
    "# w对应beta1到beta4，b对应beta0，X对应X1到X4，y对应Y（实际的值）\n",
    "def compute_gradients(X, y, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    probs = sigmoid(z)\n",
    "    error = probs - y # 对应sigmoid(z)-y\n",
    "    # 梯度计算，就是前面推导的公式\n",
    "    grad_w = np.dot(X.T, error) / len(y) # 对应(1/n)*sum((sigmoid(z)-y)*X)\n",
    "    grad_b = np.mean(error) # 对应(1/n)*sum(sigmoid(z)-y)\n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e3943971db2f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.865493200Z",
     "start_time": "2024-04-24T02:48:04.832581900Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义梯度下降函数\n",
    "def gradient_descent(X, y, w, b, alpha, iterations):\n",
    "    '''\n",
    "    :param X: 表示特征数据矩阵\n",
    "    :param y: 表示标签向量（实际值）\n",
    "    :param w: 权重向量\n",
    "    :param b: 偏置项\n",
    "    :param alpha: 学习率——就是我们的步长，每一次更新多少，是一个自己定义的参数\n",
    "    :param iterations: 迭代次数——就是我们的梯度下降要迭代多少次\n",
    "    :return: 返回最终的权重和偏置\n",
    "    '''\n",
    "    for i in range(iterations):\n",
    "        grad_w, grad_b = compute_gradients(X, y, w, b)\n",
    "        # 更新权重和偏置\n",
    "        w -= alpha * grad_w\n",
    "        b -= alpha * grad_b\n",
    "        # 每迭代一定次数打印损失\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}: Loss = {compute_loss(X, y, w, b)}\")\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e21ae74bc729bb",
   "metadata": {},
   "source": [
    "### 准备数据\n",
    "- $X_1$: 年龄\n",
    "- $X_2$: 性别（0表示女性，1表示男性）\n",
    "- $X_3$: 体重指数（BMI）\n",
    "- $X_4$: 血糖水平\n",
    "- $Y$: 是否有糖尿病（0表示没有，1表示有）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a883214b037330e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.917384200Z",
     "start_time": "2024-04-24T02:48:04.845547300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 4), (1000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备数据\n",
    "# x = df的前四列，y = df的最后一列\n",
    "X = df.iloc[:, :-1].values # iloc是通过行号来取行数据的，这里取所有行，去掉最后一列\n",
    "y = df.iloc[:, -1].values # 取所有行，取最后一列\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f11d336a3fdfafa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.934309300Z",
     "start_time": "2024-04-24T02:48:04.870522100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将X和y分为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 这里就是把X和y分为训练集和测试集，test_size=0.2表示测试集占20%，random_state=0表示随机种子，保证每次运行结果一样\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6412ef479e84955e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.954257400Z",
     "start_time": "2024-04-24T02:48:04.927328400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特征标准化——目的是让数据的均值为0，方差为1，这样可以加快梯度下降的收敛速度\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e33b3b1739fb5d32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:04.979728900Z",
     "start_time": "2024-04-24T02:48:04.944301200Z"
    }
   },
   "outputs": [],
   "source": [
    "# 初始化权重和偏置\n",
    "# 这里的w是一个一维数组，长度为X_train的列数，也就是特征的个数，这里是4\n",
    "w = np.zeros(X_train.shape[1])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "801e1cbd7e39d1f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.363391200Z",
     "start_time": "2024-04-24T02:48:04.962749700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.67694848293987\n",
      "Iteration 100: Loss = 0.31170512579698983\n",
      "Iteration 200: Loss = 0.28006450564964497\n",
      "Iteration 300: Loss = 0.26967717122142904\n",
      "Iteration 400: Loss = 0.2649489155820069\n",
      "Iteration 500: Loss = 0.26246507021700166\n",
      "Iteration 600: Loss = 0.2610482148241369\n",
      "Iteration 700: Loss = 0.26019578953398675\n",
      "Iteration 800: Loss = 0.25966368525819716\n",
      "Iteration 900: Loss = 0.2593225417497289\n",
      "Iteration 1000: Loss = 0.25909941048234025\n",
      "Iteration 1100: Loss = 0.2589512127907055\n",
      "Iteration 1200: Loss = 0.25885159858310286\n",
      "Iteration 1300: Loss = 0.2587840029545558\n",
      "Iteration 1400: Loss = 0.2587377847055575\n",
      "Iteration 1500: Loss = 0.25870598868293154\n",
      "Iteration 1600: Loss = 0.2586840050023975\n",
      "Iteration 1700: Loss = 0.25866874330605766\n",
      "Iteration 1800: Loss = 0.2586581125339943\n",
      "Iteration 1900: Loss = 0.25865068692899756\n",
      "Iteration 2000: Loss = 0.25864548820280175\n",
      "Iteration 2100: Loss = 0.2586418415797289\n",
      "Iteration 2200: Loss = 0.25863927960637956\n",
      "Iteration 2300: Loss = 0.2586374772798213\n",
      "Iteration 2400: Loss = 0.25863620795542264\n",
      "Iteration 2500: Loss = 0.25863531318147287\n",
      "Iteration 2600: Loss = 0.2586346819478313\n",
      "Iteration 2700: Loss = 0.25863423634442184\n",
      "Iteration 2800: Loss = 0.25863392161095555\n",
      "Iteration 2900: Loss = 0.2586336992106586\n",
      "Iteration 3000: Loss = 0.25863354199565103\n",
      "Iteration 3100: Loss = 0.2586334308244764\n",
      "Iteration 3200: Loss = 0.25863335219097516\n",
      "Iteration 3300: Loss = 0.25863329655937756\n",
      "Iteration 3400: Loss = 0.25863325719365426\n",
      "Iteration 3500: Loss = 0.2586332293334348\n",
      "Iteration 3600: Loss = 0.25863320961332426\n",
      "Iteration 3700: Loss = 0.25863319565339027\n",
      "Iteration 3800: Loss = 0.2586331857701633\n",
      "Iteration 3900: Loss = 0.25863317877256536\n",
      "Iteration 4000: Loss = 0.2586331738177384\n",
      "Iteration 4100: Loss = 0.25863317030914845\n",
      "Iteration 4200: Loss = 0.2586331678245427\n",
      "Iteration 4300: Loss = 0.2586331660650001\n",
      "Iteration 4400: Loss = 0.25863316481888904\n",
      "Iteration 4500: Loss = 0.2586331639363658\n",
      "Iteration 4600: Loss = 0.2586331633113288\n",
      "Iteration 4700: Loss = 0.2586331628686444\n",
      "Iteration 4800: Loss = 0.2586331625551065\n",
      "Iteration 4900: Loss = 0.25863316233303535\n",
      "Iteration 5000: Loss = 0.25863316217574606\n",
      "Iteration 5100: Loss = 0.25863316206433945\n",
      "Iteration 5200: Loss = 0.25863316198543046\n",
      "Iteration 5300: Loss = 0.25863316192953917\n",
      "Iteration 5400: Loss = 0.25863316188995095\n",
      "Iteration 5500: Loss = 0.25863316186191027\n",
      "Iteration 5600: Loss = 0.2586331618420487\n",
      "Iteration 5700: Loss = 0.2586331618279804\n",
      "Iteration 5800: Loss = 0.25863316181801566\n",
      "Iteration 5900: Loss = 0.25863316181095736\n",
      "Iteration 6000: Loss = 0.25863316180595786\n",
      "Iteration 6100: Loss = 0.25863316180241663\n",
      "Iteration 6200: Loss = 0.25863316179990825\n",
      "Iteration 6300: Loss = 0.2586331617981315\n",
      "Iteration 6400: Loss = 0.258633161796873\n",
      "Iteration 6500: Loss = 0.25863316179598156\n",
      "Iteration 6600: Loss = 0.2586331617953502\n",
      "Iteration 6700: Loss = 0.25863316179490287\n",
      "Iteration 6800: Loss = 0.25863316179458606\n",
      "Iteration 6900: Loss = 0.25863316179436174\n",
      "Iteration 7000: Loss = 0.25863316179420276\n",
      "Iteration 7100: Loss = 0.2586331617940901\n",
      "Iteration 7200: Loss = 0.2586331617940104\n",
      "Iteration 7300: Loss = 0.2586331617939539\n",
      "Iteration 7400: Loss = 0.2586331617939139\n",
      "Iteration 7500: Loss = 0.25863316179388557\n",
      "Iteration 7600: Loss = 0.2586331617938655\n",
      "Iteration 7700: Loss = 0.25863316179385126\n",
      "Iteration 7800: Loss = 0.2586331617938412\n",
      "Iteration 7900: Loss = 0.25863316179383405\n",
      "Iteration 8000: Loss = 0.258633161793829\n",
      "Iteration 8100: Loss = 0.25863316179382545\n",
      "Iteration 8200: Loss = 0.25863316179382295\n",
      "Iteration 8300: Loss = 0.2586331617938211\n",
      "Iteration 8400: Loss = 0.25863316179381984\n",
      "Iteration 8500: Loss = 0.2586331617938189\n",
      "Iteration 8600: Loss = 0.25863316179381834\n",
      "Iteration 8700: Loss = 0.25863316179381785\n",
      "Iteration 8800: Loss = 0.2586331617938175\n",
      "Iteration 8900: Loss = 0.2586331617938173\n",
      "Iteration 9000: Loss = 0.2586331617938171\n",
      "Iteration 9100: Loss = 0.25863316179381696\n",
      "Iteration 9200: Loss = 0.25863316179381696\n",
      "Iteration 9300: Loss = 0.2586331617938169\n",
      "Iteration 9400: Loss = 0.25863316179381685\n",
      "Iteration 9500: Loss = 0.25863316179381685\n",
      "Iteration 9600: Loss = 0.2586331617938168\n",
      "Iteration 9700: Loss = 0.2586331617938168\n",
      "Iteration 9800: Loss = 0.2586331617938168\n",
      "Iteration 9900: Loss = 0.2586331617938168\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "# 这里的alpha是学习率，iterations是迭代次数\n",
    "w, b = gradient_descent(X_train, y_train, w, b, alpha=0.1, iterations=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61296aafe4f322c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.380244Z",
     "start_time": "2024-04-24T02:48:05.365386200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练好模型后，我们可以使用它来进行预测\n",
    "# 预测概率\n",
    "test_probs = sigmoid(np.dot(X_test, w) + b)\n",
    "# 预测类别\n",
    "test_preds = np.where(test_probs > 0.5, 1, 0)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2be12f1660f03b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.437054100Z",
     "start_time": "2024-04-24T02:48:05.380244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25ac898d36957e0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.439050Z",
     "start_time": "2024-04-24T02:48:05.404181300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.974574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.996859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.944671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.998257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Actual  Predicted  Probability\n",
       "0        1          1     0.974574\n",
       "1        1          1     0.961203\n",
       "2        1          1     0.874061\n",
       "3        1          1     0.996859\n",
       "4        1          1     0.832528\n",
       "..     ...        ...          ...\n",
       "95       0          1     0.873257\n",
       "96       1          1     0.944671\n",
       "97       1          1     0.903943\n",
       "98       1          1     0.979292\n",
       "99       1          1     0.998257\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较真实值和预测值\n",
    "results = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': test_preds,\n",
    "    'Probability': test_probs\n",
    "})\n",
    "results.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5059d86a91f6c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.442042600Z",
     "start_time": "2024-04-24T02:48:05.412142200Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight: [1.41949725 0.42282911 0.70729532 0.90150334]\n",
      "Bias: 3.0447132565927673\n",
      "Final Accuracy: 0.885\n"
     ]
    }
   ],
   "source": [
    "# 最终的权重和偏置\n",
    "print(f\"Weight: {w}\")\n",
    "print(f\"Bias: {b}\")\n",
    "# 最终准确度\n",
    "print(f\"Final Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ffe36cfec43e67",
   "metadata": {},
   "source": [
    "# 总结\n",
    "在这个项目中，我们手动实现了逻辑回归模型的训练过程。我们首先定义了 Sigmoid 函数和对数损失函数，然后使用梯度下降法最小化损失函数。我们将这个模型应用于一个模拟数据集，其中包含患者的生理指标和是否患有糖尿病的标签。最终，我们评估了模型的准确性，并展示了预测结果。\n",
    "通过最终的结果，我们可以看到我们的模型在测试集上的准确率为 0.885，说明我们的逻辑回归模型在这个数据集上表现还是不错的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2386a57abe425",
   "metadata": {},
   "source": [
    "# 补充——采用sklearn库实现逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7004ce1241b997a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.504874600Z",
     "start_time": "2024-04-24T02:48:05.428138500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用sklearn库实现逻辑回归\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 创建逻辑回归模型\n",
    "model = LogisticRegression()\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "# 预测\n",
    "y_pred = model.predict(X_test)\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aabbf2622fe4698d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T02:48:05.507900800Z",
     "start_time": "2024-04-24T02:48:05.445047400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Model:\n",
      "Weight: [1.41949725 0.42282911 0.70729532 0.90150334]\n",
      "Bias: 3.0447132565927673\n",
      "\n",
      "Sklearn Model:\n",
      "Weight: [[1.37051336 0.40978953 0.68512706 0.87338347]]\n",
      "Bias: [2.988773]\n"
     ]
    }
   ],
   "source": [
    "# 比较调库实现的逻辑回归和手动实现的逻辑回归的权重和偏置\n",
    "print(\"Manual Model:\")\n",
    "print(f\"Weight: {w}\")\n",
    "print(f\"Bias: {b}\")\n",
    "print(\"\\nSklearn Model:\")\n",
    "print(f\"Weight: {model.coef_}\")\n",
    "print(f\"Bias: {model.intercept_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132456664f5573e",
   "metadata": {},
   "source": [
    "# 可以发现，两者的权重和偏置非常接近，这说明我们手动实现的逻辑回归模型是正确的。同时，我们也可以看到，使用sklearn库实现的逻辑回归模型的准确率与我们手动实现的模型准确率相同，这说明我们的模型在这个数据集上表现良好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
